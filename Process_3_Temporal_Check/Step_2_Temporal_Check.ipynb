{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set the proxy environment variables\n",
    "# os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "# os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import datetime\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geemap\n",
    "\n",
    "import subprocess\n",
    "from subprocess import PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append upper folder into sys-path during run time so we can\n",
    "# import our own moduls\n",
    "sys.path.append('../.')\n",
    "\n",
    "# import the temporal_consistency_check class\n",
    "from BackGround_modules.Class_5_Temporal_consistency_check import Temporal_consistency_check\n",
    "from BackGround_modules.Class_3_Calculate_the_accuracy import Accuracy_assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990_1992',\n",
       " '1993_1995',\n",
       " '1996_1998',\n",
       " '1999_2001',\n",
       " '2002_2004',\n",
       " '2005_2007',\n",
       " '2008_2010',\n",
       " '2011_2013',\n",
       " '2014_2016',\n",
       " '2017_2019',\n",
       " '2020_2022']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define basic parameters\n",
    "past_years = [f'{i}_{i+2}'for i in range(1990,2020,3)]\n",
    "future_years = ['2020_2022']\n",
    "year_range = past_years + future_years\n",
    "\n",
    "# print out the year_ranges\n",
    "year_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all regions\n",
    "region_cns = ['华东',   '东北',   '中南',    '华北',  '西北', '西南']\n",
    "region_ens = ['huadong','dongbei','zhongnan','huabei','xibei','xinan']\n",
    "\n",
    "# region for analysis\n",
    "region_cn = '西南'\n",
    "region_en = 'xinan'\n",
    "\n",
    "# region shp for exporting\n",
    "Region =  ee.FeatureCollection(\"users/wangjinzhulala/China_built_up/01_Boundary_shp/China_zone\")\\\n",
    "            .filterMetadata('NAME1','equals',region_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import COPERNICUS.urban_coverfraction layer, which will be used to masking urban classfications\n",
    "COPERNICUS_urban = ee.Image(f'COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019').select(\"urban-coverfraction\")\n",
    "China_LUCC = ee.Image(\"users/wangjinzhulala/Paper_3/01_LUCC_1990_2019_china/CLCD_v01_2019\").eq(8)\n",
    "\n",
    "past_img = ee.Image(\"users/wangjinzhulala/China_built_up/06_temporal_corrected_classification/Mosaic_only_forward_China_huadong_sample_ensemble\").gt(0)\n",
    "past_img_buffer = past_img.reduceNeighborhood(\n",
    "                              reducer='mean',\n",
    "                              kernel=ee.Kernel.circle(3)\n",
    "                            ).gt(0)\n",
    "\n",
    "# other masking layers\n",
    "dem = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "slope = ee.Terrain.slope(dem)\n",
    "precipitation = ee.Image(\"users/wangjinzhulala/China_built_up/07_supplementary_data/China_precipication_2010s\")\n",
    "\n",
    "\n",
    "# create the mask_rule_dict\n",
    "mask_rule = {'dongbei':precipitation.lt(550),\n",
    "             'huabei':slope.gt(4),\n",
    "             'huadong':slope.gt(4),\n",
    "             'xibei':dem.gt(1000).Or(precipitation.lt(500)),\n",
    "             'xinan':ee.Image(1),\n",
    "             'zhongnan':slope.gt(4)}\n",
    "\n",
    "# function to perform the masking\n",
    "def mask_img(in_img,region_name=region_en):\n",
    "    \n",
    "    # 1) first clip out the area to be masked by COPERNICUS_urban\n",
    "    img_mask_area = in_img.updateMask(mask_rule[region_name])\n",
    "    img_mask_area = img_mask_area.multiply(past_img_buffer)\n",
    "    \n",
    "    # 2) then clip out the area remain unchanged\n",
    "    img_unchange_area = in_img.updateMask(mask_rule[region_name].Not())\n",
    "    \n",
    "    # 3) mosaic the two imgs\n",
    "    merge_img = ee.ImageCollection([img_mask_area,img_unchange_area]).mosaic()\n",
    "    \n",
    "    \n",
    "    return merge_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the threshold to create ten_fold_check image out of ten random_sample_splited layers\n",
    "cross_fold_threshold = 4\n",
    "\n",
    "# path to fetch/save result\n",
    "sample_path  = 'users/wangjinzhulala/China_built_up/04_sample_train_test_split'\n",
    "img_path     = 'users/wangjinzhulala/China_built_up/05_primary_classification'\n",
    "img_out_path = 'users/wangjinzhulala/China_built_up/06_temporal_corrected_classification'\n",
    "\n",
    "# the historical temporal_checked_img\n",
    "past_checked_img = \\\n",
    " ee.Image(\"users/wangjinzhulala/China_built_up/06_temporal_corrected_classification/Mosaic_only_forward_China_huadong_sample_ensemble\")\\\n",
    " .clip(Region)\n",
    "\n",
    "past_each_year_img = {year:ee.Image(past_checked_img).gt(idx) for idx,year in enumerate(past_years[::-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification tricks\n",
    "\n",
    "\n",
    "| [Image]   | [mask]                                           |\n",
    "|:---------|:-------------------------------------------------|\n",
    "| dongbei  | {precipitation_2010s<550} & {urban_percent>20}   |\n",
    "| huabei   | {slope>4} & {urban_percent>20}                   |\n",
    "| huadong  | {slope>4} & {urban_percent>20}                   |\n",
    "| xibei    | {dem>(1000) or (precipitation<500) or (slope>4)} & {urban_percent>20}                   |\n",
    "| xinan    | {dem>3000} & {urban_percent>20}                  |\n",
    "| zhongnan | {slope>4} & {urban_percent>20}                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splipt the classification imgs into two part:\n",
    "- before 2014: do not mask \n",
    "- after 2014: do mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the masking was from start (i.e, 1990-1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_imgs = []\n",
    "\n",
    "# for year in year_range:\n",
    "    \n",
    "#     # all classification imgs with different seeds\n",
    "#     imgs = [ee.Image(f\"{img_path}/Spectrum_Normalize_Fourier_Terrain_Meterology_{region_en}_{year}_{seed}\") \n",
    "#             for seed in range(10)]\n",
    "    \n",
    "#     # sum these classifications and the img>4 will be the final result\n",
    "#     ten_folds_check = ee.ImageCollection(imgs).sum().gt(cross_fold_threshold)\n",
    "    \n",
    "#     # masking: from the classfication tricks\n",
    "#     if int(year[:4]) > 2013:\n",
    "#         # !!!! change mask_img accordingly !!!!\n",
    "#         ten_folds_check = mask_img(ten_folds_check)\n",
    "    \n",
    "#     # append masked imgs to list\n",
    "#     masked_imgs.append(ten_folds_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the masking was from middle (i.e., 2020-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something wrong here!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_imgs = list(past_each_year_img.values())[::-1]\n",
    "\n",
    "# first fetch the masked imgs from past_classifications\n",
    "for year in future_years:\n",
    "    \n",
    "    # all classification imgs with different seeds\n",
    "    imgs = [ee.Image(f\"{img_path}/Spectrum_Normalize_Fourier_Terrain_Meterology_{region_en}_{year}_{seed}\") \n",
    "            for seed in range(10)]\n",
    "    \n",
    "    # sum these classifications and the img>4 will be the final result\n",
    "    ten_folds_check = ee.ImageCollection(imgs).sum().gt(cross_fold_threshold).rename(\"remapped\")\n",
    "    \n",
    "    # masking: from the classfication tricks\n",
    "    if int(year[:4]) > 2013:\n",
    "        # !!!! change mask_img accordingly !!!!\n",
    "        ten_folds_check = mask_img(ten_folds_check).gt(0).rename(\"remapped\")\n",
    "    \n",
    "    # append masked imgs to list\n",
    "    masked_imgs.append(ten_folds_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the img to map\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(Region,10)\n",
    "Map.add_basemap('HYBRID')\n",
    "\n",
    "# Map.addLayer(masked_imgs[0], {'min':0,'max':1},'Iter_1')\n",
    "# Map.addLayer(masked_imgs[2], {'min':0,'max':1},'Iter_3')\n",
    "# Map.addLayer(masked_imgs[4], {'min':0,'max':1},'Iter_5')\n",
    "# Map.addLayer(masked_imgs[6], {'min':0,'max':1},'Iter_7')\n",
    "# Map.addLayer(masked_imgs[8], {'min':0,'max':1},'Iter_9')\n",
    "Map.addLayer(masked_imgs[9], {'min':0,'max':1},'Iter_10')\n",
    "Map.addLayer(masked_imgs[-1], {'min':0,'max':1},'Iter_11')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform temporal consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the checked results\n",
    "Iteration_num = 10\n",
    "Window_size = [2,3,4,5,6]\n",
    "modes = ['only_forward'] #['backward_forward','forward_backward','only_forward','only_backward']\n",
    "\n",
    "Check_df_list = []\n",
    "\n",
    "for window in Window_size:\n",
    "    for mode in modes:\n",
    "    \n",
    "        # get the checked classification imgs, here they are stored in a dictionary\n",
    "        Iter_temporal_check_instaces = Temporal_consistency_check(masked_imgs,window,Iteration_num)\\\n",
    "                                        .Iterate_the_check(mode = mode)\n",
    "\n",
    "        # convert the Iter_temporal_check_instaces to a Dataframe\n",
    "        Check_instances = pd.DataFrame(Iter_temporal_check_instaces).T\n",
    "        Check_instances.columns = year_range\n",
    "\n",
    "        # Add the window-size and iteration-num to the df\n",
    "        Check_instances['Mode'] = [mode] * Iteration_num\n",
    "        Check_instances['Window'] = [window] * Iteration_num\n",
    "        Check_instances['Iteration'] = range(1,Iteration_num+1)\n",
    "\n",
    "        # Reorder the columns\n",
    "        cols = Check_instances.columns\n",
    "        Check_instances = Check_instances[list(cols[-3:]) + list(cols[:-3])] \n",
    "\n",
    "        # add the checked df into list\n",
    "        Check_df_list.append(Check_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge all instace_df together\n",
    "Check_df = pd.concat(Check_df_list)\n",
    "Check_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the Check_sample into long format so each row is an observation with spicific window-iteration-year condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach Check_sample to the Check_instaces_long \n",
    "Check_instaces_long = pd.DataFrame(Check_df.set_index(['Mode','Window','Iteration']).stack()).reset_index()\n",
    "Check_instaces_long.columns = ['Mode','Window','Iteration','year','Image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Check_instaces_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Mosaic image from all temporal-checked imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mosaic_img = {}\n",
    "\n",
    "for window in Window_size:\n",
    "    for iteration in range(1,Iteration_num+1):\n",
    "        # get the temporal_checked imgs\n",
    "        Temporal_checked_select  = Check_instaces_long[(Check_instaces_long['Iteration'] == iteration)&\\\n",
    "                                                       (Check_instaces_long['Window']    == window)     ]['Image'].values\n",
    "#         # reclassify the classification img\n",
    "#         Temporal_checked_select_re = [img.remap([0,1],[0,len(Temporal_checked_select)-i]) \n",
    "#                                       for i,img in enumerate(Temporal_checked_select)]\n",
    "\n",
    "#         # mosaic all checked imgs together\n",
    "#         Classification_Mosaic = ee.ImageCollection(Temporal_checked_select_re).max()\n",
    "\n",
    "        Classification_Mosaic = ee.Image(0)\n",
    "        for img in Temporal_checked_select:   \n",
    "            Classification_Mosaic = Classification_Mosaic.add(img)\n",
    "\n",
    "        \n",
    "        # store the img to dict\n",
    "        Mosaic_img[(window,iteration)] = [Classification_Mosaic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the mosaic classification into a dataframe and format the column names\n",
    "Mosaic_df = pd.DataFrame(Mosaic_img).T\n",
    "\n",
    "# format the column names\n",
    "Mosaic_df_reindex = Mosaic_df.reset_index()\n",
    "Mosaic_df_reindex.columns = ['window','iteration','mosaic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the checked img to the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color_ramp for Mosaic img\n",
    "Mosaic_VIS = {'min':0,\n",
    "              'max':len(masked_imgs),\n",
    "              \"palette\":[\"000000\",\"3288bd\",\"66c2a5\",\"abdda4\",\n",
    "                         \"e6f598\",\"ffffbf\",\"fee08b\",\"fdae61\",\n",
    "                         \"f46d43\",\"d53e4f\",\"9e0142\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filter parameters\n",
    "mode = 'only_forward' # 'backward_forward' |'forward_backward'|'only_forward'|'only_backward'\n",
    "window_len = 3\n",
    "iterantion_num = 9\n",
    "year_idx = '1990_1992'\n",
    "\n",
    "\n",
    "# get the temporal checked imgs\n",
    "Temporal_checkd = Check_instaces_long[(Check_instaces_long['Mode']   == mode)&\n",
    "                                      (Check_instaces_long['Window'] == window_len)&\n",
    "                                      (Check_instaces_long['year']   == year_idx)]\n",
    "\n",
    "# get the checked img with different iteration\n",
    "img_year_window_filtered = [Temporal_checkd[Temporal_checkd['Iteration'] == i]['Image'].values[0] \n",
    "                            for i in range(1,Iteration_num + 1)]\n",
    "\n",
    "# get the \n",
    "Mosaic_image = Mosaic_df_reindex[(Mosaic_df_reindex['window'] == window_len) \\\n",
    "                               &(Mosaic_df_reindex['iteration'] == iterantion_num)]['mosaic'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the img to map\n",
    "Map = geemap.Map()\n",
    "Map.centerObject(Region,10)\n",
    "Map.add_basemap('HYBRID')\n",
    "\n",
    "# create a color ramp for mosaiced img\n",
    "Mosaic_VIS = {\"opacity\":1,'min':0,\"max\":10,\n",
    "              \"palette\":[\"000000\",\"3288bd\",\"66c2a5\",\"abdda4\",\"e6f598\",\n",
    "                         \"ffffbf\",\"fee08b\",\"fdae61\",\"f46d43\",\"d53e4f\",\"9e0142\"]}\n",
    "\n",
    "# add image to map\n",
    "\n",
    "# Map.addLayer(img_year_window_filtered[0], {'min':0,'max':1},'Iter_1')\n",
    "# Map.addLayer(img_year_window_filtered[2], {'min':0,'max':1},'Iter_3')\n",
    "# Map.addLayer(img_year_window_filtered[4], {'min':0,'max':1},'Iter_5')\n",
    "# Map.addLayer(img_year_window_filtered[6], {'min':0,'max':1},'Iter_7')\n",
    "# Map.addLayer(img_year_window_filtered[8], {'min':0,'max':1},'Iter_9')\n",
    "# Map.addLayer(img_year_window_filtered[9], {'min':0,'max':1},'Iter_10')\n",
    "\n",
    "Map.addLayer(Mosaic_image,                  Mosaic_VIS,       'Mosaic')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export the temporal checked classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export each year's classification img of temporal_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we find that temporcal checked img with \n",
    "# \"Mode=forward_backward\", \"Iteration=9\", \"Window=3\"\n",
    "# are the best quality imgs\n",
    "\n",
    "Target_temporal_checked = Check_instaces_long[(Check_instaces_long['Mode']      == 'only_forward')&\\\n",
    "                                              (Check_instaces_long['Iteration'] == 9)&\\\n",
    "                                              (Check_instaces_long['Window']    == 3)]\n",
    "\n",
    "Target_mosaci_img = Mosaic_df_reindex[(Mosaic_df_reindex['window'] == 3) \\\n",
    "                                     &(Mosaic_df_reindex['iteration'] == 9)]['mosaic'].values[0].toInt8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the mosaiced classification img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the result\n",
    "export_name = f'Mosaic_only_forward_{region_en}'\n",
    "\n",
    "\n",
    "#export to GEE assset\n",
    "task = ee.batch.Export.image.toAsset(    \n",
    "                                        image= Classification_Mosaic,\n",
    "                                        description=export_name,\n",
    "                                        assetId=f'{img_out_path}/{export_name}',\n",
    "                                        region=Region.geometry().bounds(),\n",
    "                                        scale=30,\n",
    "                                        maxPixels=int(1e13)\n",
    ")\n",
    "\n",
    "# # to cloud_storage\n",
    "# task = ee.batch.Export.image.toCloudStorage(image = Target_mosaci_img,\n",
    "#                                             description=export_name,\n",
    "#                                             bucket='north_china_plain',\n",
    "#                                             fileNamePrefix=f'China_built/{export_name}',\n",
    "#                                             region=Region.geometry().bounds(),\n",
    "#                                             scale=30,\n",
    "#                                             maxPixels=int(1e13),\n",
    "#                                             skipEmptyTiles=True)\n",
    "\n",
    "task.start()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the accuracy after the temporal check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the untouched control points\n",
    "Check_sample = [ee.FeatureCollection(f'{sample_path}/Grid_select_{region_en}_{year}') for year in year_range]\n",
    "\n",
    "\n",
    "# Sample_Sentinel = ee.FeatureCollection(\"users/wangjinzhulala/North_China_Plain_Python/Sample_Points/Sentinel_2017_2019\")\\\n",
    "#                     .randomColumn('split', 101)\\\n",
    "#                     .filterMetadata('split','less_than',0.25)\n",
    "\n",
    "# # create sample list for accuracy assesment\n",
    "# Check_sample = [Sample_Landsat] * len_landsat_1990_2013 + \\\n",
    "#                [Sample_Sentinel] * len_sentinel_2014_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'2020_2022'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m sample \u001b[38;5;241m=\u001b[39m Check_sample[i]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# get the classificaiton, rename the band so we can use Accuracy_assesment module\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m img_my   \u001b[38;5;241m=\u001b[39m ee\u001b[38;5;241m.\u001b[39mImage(mosaic_img\u001b[38;5;241m.\u001b[39mgte(\u001b[43mMosaic_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43myear\u001b[49m\u001b[43m]\u001b[49m))\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# extract the img pixel value to test_sample\u001b[39;00m\n\u001b[0;32m     24\u001b[0m test_with_My   \u001b[38;5;241m=\u001b[39m img_my\u001b[38;5;241m.\u001b[39msampleRegions(  collection \u001b[38;5;241m=\u001b[39m sample, \n\u001b[0;32m     25\u001b[0m                                         properties \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuilt\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     26\u001b[0m                                         scale      \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     27\u001b[0m                                         tileScale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: '2020_2022'"
     ]
    }
   ],
   "source": [
    "# create a dictionary stores are year-value pairs\n",
    "Mosaic_value = dict(zip(year_range,range(Iteration_num,0,-1)))\n",
    "\n",
    "\n",
    "# calculate the accuracy using the untouched control samples\n",
    "Accuracy_dict = {}\n",
    "\n",
    "# double loop to calculate the accuracy\n",
    "for idx,row in Mosaic_df_reindex.iterrows():\n",
    "    \n",
    "    window     = row[0]\n",
    "    iteration  = row[1]\n",
    "    mosaic_img = row[2]\n",
    "\n",
    "    for i,year in enumerate(year_range):\n",
    "\n",
    "        # get the img and untouched sample\n",
    "        sample = Check_sample[i]\n",
    "\n",
    "        # get the classificaiton, rename the band so we can use Accuracy_assesment module\n",
    "        img_my   = ee.Image(mosaic_img.gte(Mosaic_value[year])).rename('classification')\n",
    "\n",
    "        # extract the img pixel value to test_sample\n",
    "        test_with_My   = img_my.sampleRegions(  collection = sample, \n",
    "                                                properties = ['Built'], \n",
    "                                                scale      = 30,\n",
    "                                                tileScale = 6)\n",
    "\n",
    "\n",
    "        # compute the accuracy and put them into a df\n",
    "        Accuracy_dict[(window,iteration,year)] = test_with_My.errorMatrix('Built','classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute the accuracy from GEE server in chunck\n",
    "chunck_size = 10\n",
    "\n",
    "# get all accuracy instances\n",
    "error_matrix_instances = [v for k,v in Accuracy_dict.items()]\n",
    "\n",
    "# calculate the accuracy by chunck\n",
    "acc_value = []\n",
    "\n",
    "# set flags to report the process\n",
    "flag = 0 \n",
    "flag_total = len(error_matrix_instances)/chunck_size\n",
    "\n",
    "# loop through the chunk to get accuracy\n",
    "for i in range(0,len(error_matrix_instances),chunck_size):\n",
    "    \n",
    "    chunck = error_matrix_instances[i:i+chunck_size]\n",
    "    acc = ee.List(chunck).map(lambda mat:ee.ConfusionMatrix(mat).accuracy()).getInfo()\n",
    "    acc_value.extend(acc)\n",
    "    \n",
    "    # print out the process\n",
    "    flag = flag + 1 \n",
    "    print(f'The operation of {flag}/{int(flag_total)} is complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sotre the accuracy into a dataframe and formate thedf\n",
    "acc_k = [k for k,v in Accuracy_dict.items()]\n",
    "acc_df = pd.DataFrame(dict( (k,[v *100]) for k,v in zip(acc_k,acc_value))).T\n",
    "\n",
    "# format the df\n",
    "Checked_acc = acc_df.reset_index()\n",
    "Checked_acc.columns = ['window','iteration','year','accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the accuracy after temporal-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the accuracy to disk\n",
    "Checked_acc.to_csv(f'./Result/Temporal_check_acc_{region_en}.csv',index=False)\n",
    "\n",
    "# load the acc_df\n",
    "Checked_acc = pd.read_csv(f'./Result/Temporal_check_acc_{region_en}.csv')\n",
    "\n",
    "# concert these columns into category, so we can make figures\n",
    "Checked_acc['iteration'] = Checked_acc['iteration'].astype('category')\n",
    "Checked_acc['year'] = Checked_acc['year'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Checked_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe that window-size of 3 is the optimun size, because the bigger the window size, the more imgs wouldn't be temporal corrected. So we choose 3 as window size even though 5 look like the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.lineplot(data = Checked_acc,x='window',y='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe that  9 iterations  achieved a stable accuracy. here we neglect the seemingly high accuracy at lower iterations because we care more about stable performance, which is the key of temporal correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.lineplot(data = Checked_acc[Checked_acc['window'] == 3],\n",
    "             x='iteration',\n",
    "             y='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe that the temporal correction have incresed the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) import the original accuracy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the original acc_df\n",
    "Sentinel_Acc = pd.read_csv('../Process_1_GEE_Python_Classification/Sub_Process_7_Classification_on_img/Result/Classification_Accuracy_landsat_sentinel.csv')\n",
    "Landsat_Acc  = pd.read_csv('../Process_1_GEE_Python_Classification/Sub_Process_7_Classification_on_img/Result/Classification_Accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification only use landsat as input\n",
    "landsat_col = ['Landsat_1990_1992', 'Landsat_1993_1995', 'Landsat_1996_1998','Landsat_1999_2001',\n",
    "               'Landsat_2002_2004', 'Landsat_2005_2007','Landsat_2008_2010', 'Landsat_2011_2013']\n",
    "\n",
    "# concatenate all accuracy into one df\n",
    "Original_landsat = Landsat_Acc[Landsat_Acc['year'].isin(landsat_col)]\n",
    "Original_acc = pd.concat([Original_landsat,Sentinel_Acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formeting the original addcuracy_df\n",
    "Original_acc['Year'] = Original_acc['year'].apply(lambda x: x[-9:])\n",
    "Original_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) get the 10-folds threshold correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "Threshold_acc_df = pd.read_csv('../Process_1_GEE_Python_Classification/Sub_Process_8_Determine_the_threshold_for_the_sum_of_10_random_classification/Result/Threshold_acc_df.csv')\n",
    "\n",
    "# make the threshold categorical so we can make figures\n",
    "Threshold_acc_df['Threshold'] = Threshold_acc_df['Threshold'].astype('category')\n",
    "Threshold_acc_df['Year'] = Threshold_acc_df['Year'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) make a figure to compare all the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# original accuracy\n",
    "sns.lineplot(data = Original_acc,x = 'Year',y='Overall_ACC',label = 'Original')\n",
    "\n",
    "\n",
    "# threshold correction\n",
    "sns.lineplot(data = Threshold_acc_df[Threshold_acc_df['Threshold']==4],\n",
    "             x='Year',\n",
    "             y='Accuracy',\n",
    "            label = 'Threshold Correction')\n",
    "\n",
    "\n",
    "# temporal correction\n",
    "sns.lineplot(data = Checked_acc[((Checked_acc['window']==3)&\n",
    "                                 (Checked_acc['iteration']==9))],\n",
    "             x='year',\n",
    "             y='accuracy',\n",
    "            label = 'Temporal Correction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
